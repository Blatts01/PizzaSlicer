{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pizza slicer based on Yolov3 and Ultra96-V2 board\n",
    "\n",
    "The last section explains the Pizza Slicer code. The whole program is written in python3 and usees a jupyter notebook to control the program.\n",
    "The first three cells cover all Python imports and FPGA / DPU file loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Python imports\n",
    "from pynq import allocate, Overlay\n",
    "from pynq.lib.video import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib notebook\n",
    "import operator\n",
    "import time as t\n",
    "import PIL.Image\n",
    "import cv2\n",
    "from skimage.transform import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the FGPA bin file, just for testing \n",
    "base_overlay = Overlay(\"pizza.bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the FGPA bin file\n",
    "from pynq_dpu import DpuOverlay\n",
    "overlay = DpuOverlay(\"pizza.bit\")\n",
    "overlay.load_model(\"pizza.elf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Slicing masks. The problem of finding the best angle is solved geometrically. I used 180 masks, which cut the 320 x 320 image into two parts. The comment code, enable matplotlib plots which draw the slicing maks. \n",
    "\n",
    "\n",
    "The image show the slicing maks for 10; 70 and 145 degree \n",
    "\n",
    "![Slicing Maks](img/SlicingMasks.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa463a8b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generate the slicing masks, from 0 to 180 degree cutting angle \n",
    "base_array_zero = np.zeros((640, 320))\n",
    "base_array_once = np.full((640, 320),255, dtype=np.uint8)\n",
    "\n",
    "base_array = np.hstack([base_array_zero, base_array_once])\n",
    "\n",
    "mask_dict = {}\n",
    "\n",
    "for i in range(0, 181):\n",
    "    tmp_rot = rotate(base_array, i, center=(320, 320))\n",
    "    tmp_mask = tmp_rot[160:480, 160:480]\n",
    "    uint_mask = tmp_mask.astype('uint8')\n",
    "    mask_dict[i] = uint_mask\n",
    "\n",
    "#print(uint_mask.shape)\n",
    "#print(uint_mask)    \n",
    "\n",
    "#fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "#axs[0].imshow(mask_dict[10])\n",
    "#axs[1].imshow(mask_dict[70])\n",
    "#axs[2].imshow(mask_dict[145])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output images \n",
    "\n",
    "### Base Background \n",
    "![Slicing Maks](Pizza_1.png)\n",
    "\n",
    "### \"Cut Here\" Background\n",
    "![Slicing Maks](Pizza_1_cut_here.png)\n",
    "\n",
    "### \"rotated arrows\"\n",
    "![Slicing Maks](Pizza_1_arrow.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images for projection \n",
    "out_img_bg = cv2.imread(\"Pizza_1.png\")\n",
    "out_img_cut_here = PIL.Image.open(\"Pizza_1_cut_here.png\")\n",
    "out_img_arrow = PIL.Image.open(\"Pizza_1_arrow.png\")\n",
    "#out_img_bg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading anchors for Yolov3 network. Yolov3 uses a fixed set of anchors to generate the bounding boxes. These anchors are used in the evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolov3 anchro list, used to find the bounding boxes\n",
    "\n",
    "anchor_list = [10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326]\n",
    "anchor_float = [float(x) for x in anchor_list]\n",
    "anchors = np.array(anchor_float).reshape(-1, 2)\n",
    "\n",
    "KERNEL_CONV=\"tf_yolov3\"\n",
    "CONV_INPUT_NODE=\"conv2d_1_convolution\"\n",
    "CONV_OUTPUT_NODE1=\"conv2d_59_convolution\"\n",
    "CONV_OUTPUT_NODE2=\"conv2d_67_convolution\"\n",
    "CONV_OUTPUT_NODE3=\"conv2d_75_convolution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation function converts the Yolov3 output to boxes, classes and scores. This is needed because all other steps need the output in bounding boxes format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolov3 output evaluation function \n",
    "def evaluate(yolo_outputs, image_shape, class_names, anchors):\n",
    "    score_thresh = 0.2\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    input_shape = np.shape(yolo_outputs[0])[1 : 3]\n",
    "    input_shape = np.array(input_shape)*32\n",
    "\n",
    "    for i in range(len(yolo_outputs)):\n",
    "        _boxes, _box_scores = boxes_and_scores(\n",
    "            yolo_outputs[i], anchors[anchor_mask[i]], len(class_names), \n",
    "            input_shape, image_shape)\n",
    "        boxes.append(_boxes)\n",
    "        box_scores.append(_box_scores)\n",
    "    boxes = np.concatenate(boxes, axis = 0)\n",
    "    box_scores = np.concatenate(box_scores, axis = 0)\n",
    "\n",
    "    mask = box_scores >= score_thresh\n",
    "    boxes_ = []\n",
    "    scores_ = []\n",
    "    classes_ = []\n",
    "    for c in range(len(class_names)):\n",
    "        class_boxes_np = boxes[mask[:, c]]\n",
    "        class_box_scores_np = box_scores[:, c]\n",
    "        class_box_scores_np = class_box_scores_np[mask[:, c]]\n",
    "        nms_index_np = nms_boxes(class_boxes_np, class_box_scores_np) \n",
    "        class_boxes_np = class_boxes_np[nms_index_np]\n",
    "        class_box_scores_np = class_box_scores_np[nms_index_np]\n",
    "        classes_np = np.ones_like(class_box_scores_np, dtype = np.int32) * c\n",
    "        boxes_.append(class_boxes_np)\n",
    "        scores_.append(class_box_scores_np)\n",
    "        classes_.append(classes_np)\n",
    "    boxes_ = np.concatenate(boxes_, axis = 0)\n",
    "    scores_ = np.concatenate(scores_, axis = 0)\n",
    "    classes_ = np.concatenate(classes_, axis = 0)\n",
    "\n",
    "    return boxes_, scores_, classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDMI setup for camera input and projector output. The code is compatible with the PYNQ HDMI interface. A custom in/output can be set up at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup HDMI\n",
    "hdmi_in = base_overlay.video.hdmi_in\n",
    "hdmi_out = base_overlay.video.hdmi_out\n",
    "hdmi_in.configure(PIXEL_RGB)\n",
    "\n",
    "#hdmi_in.configure()\n",
    "hdmi_out.configure(hdmi_in.mode)\n",
    "\n",
    "hdmi_in.start()\n",
    "hdmi_out.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Print default output on projector\n",
    "outframe = hdmi_out.newframe()\n",
    "cv2.cvtColor(out_img_bg, cv2.COLOR_RGB2BGR,dst=outframe)\n",
    "hdmi_out.writeframe(outframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Input framerate\n",
    "numframes = 600\n",
    "start = t.time()\n",
    "\n",
    "for _ in range(numframes):\n",
    "    frame = hdmi_in.readframe()\n",
    "\n",
    "    \n",
    "end = t.time()\n",
    "print(\"Frames per second:  \" + str(numframes / (end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show input frame from camera\n",
    "frame = hdmi_in.readframe()\n",
    "#image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "image_Pil = PIL.Image.fromarray(frame)\n",
    "#image?\n",
    "image_Pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Image preparation\n",
    "\n",
    "The input image from the camera is cropped in the first step, to select the region of interest (ROI). My GoPro Camera outputs a 720p image, so the ROI is set to 440 x 440 pixels, cropping is done in the software. This is useful if your camera is not fixed at the projector and some adjustments are needed to fit the pizza in the middle of the ROI. The ROI image data is transferred to the FPGA image resizer, the data transfer is done by some DMA tasks. To enable data transfer from the python memory to the PL memory, the pynq.buffer is used. The output from the PL image resizer is used as  DPU input. The source code from PL resizer is forked from this PYNQ-Helloworld GitHub repo. The PL resizer can handle custom in/ output images, the settings for in/output images can be set via registers. Four register values must be set: 0x10 : input image height ; 0x18 input image width; 0x20 output image height; 0x28 output image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup PL resizer \n",
    "\n",
    "#Setup image resizer dmas \n",
    "resize_dma = base_overlay.axi_dma_0\n",
    "resizer_accelerator = base_overlay.resize_accel_0\n",
    "\n",
    "# configure resizer registers \n",
    "resizer_accelerator.write(0x10, 440)\n",
    "resizer_accelerator.write(0x18, 440)\n",
    "resizer_accelerator.write(0x20, 320)\n",
    "resizer_accelerator.write(0x28, 320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buffers \n",
    "hdmi_input = allocate(shape=(720, 1280, 3),dtype=np.uint8)\n",
    "hdmi_input_crop = allocate(shape=(440, 440, 3),dtype=np.uint8)\n",
    "hdmi_resized = allocate(shape=(320, 320, 3), dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input, crop and use PL resizer for DPU image size \n",
    "input_frame = np.array(hdmi_in.readframe())\n",
    "resizer_input = input_frame[220:660,400:840]\n",
    "resizer_input.shape\n",
    "hdmi_input_crop[:] = resizer_input\n",
    "hdmi_input_crop.sync_to_device()\n",
    "\n",
    "resize_dma.sendchannel.transfer(hdmi_input_crop)\n",
    "resize_dma.recvchannel.transfer(hdmi_resized)    \n",
    "resizer_accelerator.write(0x00,0x81) # start\n",
    "resize_dma.sendchannel.wait()\n",
    "resize_dma.recvchannel.wait()\n",
    "hdmi_resized.sync_from_device()\n",
    "\n",
    "resized_input_array = np.array(hdmi_resized)\n",
    "resized_input_array.shape\n",
    "\n",
    "pil_dpu_image = PIL.Image.fromarray(resized_input_array)\n",
    "pil_dpu_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is used for recording training images \n",
    "from datetime import timezone \n",
    "import datetime \n",
    "# Getting the current date  \n",
    "# and time \n",
    "dt = datetime.datetime.now() \n",
    "  \n",
    "utc_time = dt.replace(tzinfo = timezone.utc) \n",
    "utc_timestamp = utc_time.timestamp() \n",
    "int_utc_time = int(utc_timestamp)\n",
    "\n",
    "# read input, crop and use PL resizer for DPU image size \n",
    "input_frame = np.array(hdmi_in.readframe())\n",
    "resizer_input = input_frame[220:660,400:840]\n",
    "resizer_input.shape\n",
    "hdmi_input_crop[:] = resizer_input\n",
    "hdmi_input_crop.sync_to_device()\n",
    "\n",
    "resize_dma.sendchannel.transfer(hdmi_input_crop)\n",
    "resize_dma.recvchannel.transfer(hdmi_resized)    \n",
    "resizer_accelerator.write(0x00,0x81) # start\n",
    "resize_dma.sendchannel.wait()\n",
    "resize_dma.recvchannel.wait()\n",
    "hdmi_resized.sync_from_device()\n",
    "\n",
    "resized_input_array = np.array(hdmi_resized)\n",
    "resized_input_array.shape\n",
    "\n",
    "pil_dpu_image = PIL.Image.fromarray(resized_input_array)\n",
    "pil_dpu_image.save(str(int_utc_time) + \"_pizza.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPU setup\n",
    "\n",
    "DPU init and DPU Task starting. The evaluation function converts the raw Yolov3 network output to bounding boxes format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init n2cube for DPU handling\n",
    "import n2cube \n",
    "n2cube.dpuOpen()\n",
    "\n",
    "kernel = n2cube.dpuLoadKernel(KERNEL_CONV)\n",
    "task = n2cube.dpuCreateTask(kernel, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare input data and run dpu task\n",
    "\n",
    "input_len = n2cube.dpuGetInputTensorSize(task, CONV_INPUT_NODE)\n",
    "opencvImage = cv2.cvtColor(np.array(resized_input_array), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "n2cube.dpuSetInputTensorInHWCFP32(task, CONV_INPUT_NODE, opencvImage, input_len)\n",
    "n2cube.dpuRunTask(task)\n",
    "\n",
    "#plt.imshow(cv2.cvtColor(opencvImage, cv2.COLOR_BGR2RGB))\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Test image or use web cam input #### \n",
    "\n",
    "input_len = n2cube.dpuGetInputTensorSize(task, CONV_INPUT_NODE)\n",
    "opencvImage = cv2.imread(\"pizza_5.jpg\")\n",
    "#opencvImage = cv2.imread(\"1606875273_pizza.jpg\")\n",
    "opencvImage.shape\n",
    "\n",
    "n2cube.dpuSetInputTensorInHWCFP32(task, CONV_INPUT_NODE, opencvImage, input_len)\n",
    "n2cube.dpuRunTask(task)\n",
    "\n",
    "#plt.imshow(cv2.cvtColor(opencvImage, cv2.COLOR_BGR2RGB))\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_sbbox_size = n2cube.dpuGetOutputTensorSize(task, CONV_OUTPUT_NODE1)\n",
    "conv_out1 = n2cube.dpuGetOutputTensorInHWCFP32(task, CONV_OUTPUT_NODE1, \n",
    "                                               conv_sbbox_size)\n",
    "conv_out1 = np.reshape(conv_out1, (1, 13, 13, 75))\n",
    "\n",
    "conv_mbbox_size = n2cube.dpuGetOutputTensorSize(task, CONV_OUTPUT_NODE2)\n",
    "conv_out2 = n2cube.dpuGetOutputTensorInHWCFP32(task, CONV_OUTPUT_NODE2, \n",
    "                                               conv_mbbox_size)\n",
    "conv_out2 = np.reshape(conv_out2, (1, 26, 26, 75))\n",
    "\n",
    "conv_lbbox_size = n2cube.dpuGetOutputTensorSize(task, CONV_OUTPUT_NODE3)\n",
    "conv_out3 = n2cube.dpuGetOutputTensorInHWCFP32(task, CONV_OUTPUT_NODE3, \n",
    "                                               conv_lbbox_size)\n",
    "conv_out3 = np.reshape(conv_out3, (1, 52, 52, 75))\n",
    "\n",
    "yolo_outputs = [conv_out1, conv_out2, conv_out3]\n",
    "\n",
    "outs = evaluate(yolo_outputs, image_size, class_names, anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pepperoni heatmap creation\n",
    "\n",
    "The output from the evaluation function is passed to a sorting process. Duplicate pizza detections are removed, only the highest confidence for pizza is used. Pepperoni detections are used if the confidence is higher than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pepperoni heatmap \n",
    "\n",
    "base_array = np.zeros((320,320), dtype=np.uint8)\n",
    "pepperoni_map = ()\n",
    "\n",
    "#Image dimensions\n",
    "height = 320\n",
    "width = 320\n",
    "channels = 3\n",
    "\n",
    "\n",
    "class_ids=[]\n",
    "confidences=[]\n",
    "ellipses=[]\n",
    "\n",
    "dnn_detection={}\n",
    "cnt=0\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            center_x= int(detection[0]*width)\n",
    "            center_y= int(detection[1]*height)\n",
    "            w = int((detection[2]*width)/2)\n",
    "            h = int((detection[3]*height)/2)\n",
    "            #x=int(center_x - w/2)\n",
    "            #y=int(center_y - h/2)\n",
    "            \n",
    "            x=int(center_x)\n",
    "            y=int(center_y)\n",
    "            \n",
    "            #append object to dict \n",
    "            dnn_detection[str(cnt)] = ([x,y,w,h],float(confidence),class_id)\n",
    "            cnt = cnt +1 \n",
    "            #ellipses.append([x,y,w,h]) \n",
    "            #confidences.append(float(confidence))\n",
    "            #class_ids.append(class_id) \n",
    "\n",
    "print(\"found {} objects\".format(len(dnn_detection)))\n",
    "#dnn_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elliptic approximation\n",
    "\n",
    "Based on the sorting out process, a virtual pizza is drawn in a 320 x320 uint8 array. The bounding box coordinates are used to draw an ellipse inside the bounding box. The inside of the ellipse is filled with 1 the outside with 0. Every pepperoni approximation is added to the 320 x 320 base array. The base array element is a pizza bounding box approximation. Pepperoni detections outside the pizza base array are ignored. The result is a pepperoni heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pizza has id 1, pepperoni has id 0 \n",
    "#sort the dection dict \n",
    "\n",
    "found_pizza = False \n",
    "pepperoni_map = list()\n",
    "\n",
    "sorted_dict = dict( sorted(dnn_detection.items(),key=operator.itemgetter(1)))\n",
    "#print(sorted_dict)\n",
    "for item in sorted_dict:\n",
    "    #print(sorted_dict[item])\n",
    "    tmp_item = sorted_dict[item]\n",
    "    \n",
    "    if tmp_item[2] == 1 and not found_pizza:\n",
    "        found_pizza = True\n",
    "        pizza_base = cv2.ellipse(base_array, (tmp_item[0][0],tmp_item[0][1]),(tmp_item[0][2],tmp_item[0][3]), 0, 0, 360,1,-1)\n",
    "    if tmp_item[2] == 0:\n",
    "        tmp_base = np.zeros((320,320), dtype=np.uint8)\n",
    "        pepperoni_map.append(cv2.ellipse(tmp_base,(tmp_item[0][0],tmp_item[0][1]),(tmp_item[0][2],tmp_item[0][3]), 0, 0, 360,1,-1))\n",
    "\n",
    "pizza_heatmap = pizza_base\n",
    "        \n",
    "#remove pepperoni detection outside the pizza         \n",
    "for pepperoni in pepperoni_map:\n",
    "    tmp_map = cv2.bitwise_and(pizza_base,pepperoni)\n",
    "    pizza_heatmap = pizza_heatmap + tmp_map \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pepperoni heatmap, overlaying pepperoni slices are also simulated (brighter areas). That is one reason, why an object detection network is used instead of a segmentation network. With the object detection network and the elliptic area approximation overlaying pepperonis can be simulated.\n",
    "\n",
    "\n",
    "### Testimage Yolov3 heatmap\n",
    "![Slicing Maks](img/TestimageYolov3compare.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the images, detection versus compare yolov2 input \n",
    "\n",
    "# make the images bigger for browser rendering \n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(pizza_heatmap)\n",
    "axs[1].imshow(cv2.cvtColor(opencvImage,cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D pepperoni heatmap of test image\n",
    "\n",
    "![Slicing Maks](img/TestimageHeatmap.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nice 3d view of the heatmap\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.arange(0,320,1)\n",
    "y = np.arange(0,320,1)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "X.shape\n",
    "Z = pizza_heatmap\n",
    "\n",
    "mycmap = plt.get_cmap('YlOrRd')\n",
    "ax1.set_title('Pepperoni heatmap')\n",
    "surf1 = ax1.plot_surface(X, Y, Z, cmap=mycmap)\n",
    "fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the optimal cutting angel \n",
    "# I solve this problem geometrical applying a \"mask\" to the heatmap and do a binary search for the best cutting angle\n",
    "heat_map_sum = np.sum(pizza_heatmap)\n",
    "half_heat_map_sum = heat_map_sum/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do a binary search for prefect slice angle, i search the pos/neg crossing for (half_heat_map_sum - sum of current angle) \n",
    "start = 0\n",
    "step = 0\n",
    "low = 0\n",
    "mid = 90\n",
    "high = 180\n",
    "end = 180\n",
    "diff = -1\n",
    "\n",
    "tmp_sum_heatmap = np.zeros((320,320))\n",
    "\n",
    "while (low != mid):  \n",
    "    np.bitwise_and(pizza_heatmap,mask_dict[low],tmp_sum_heatmap)\n",
    "    uint_tmp_sum_heatmap_low = tmp_sum_heatmap.astype('uint8')\n",
    "    tmp_sum_low = np.sum(uint_tmp_sum_heatmap_low)\n",
    "    diff_low = half_heat_map_sum - tmp_sum_low\n",
    "    \n",
    "    np.bitwise_and(pizza_heatmap,mask_dict[mid],tmp_sum_heatmap)\n",
    "    uint_tmp_sum_heatmap_mid = tmp_sum_heatmap.astype('uint8')\n",
    "    tmp_sum_mid = np.sum(uint_tmp_sum_heatmap_mid)\n",
    "    diff_mid = half_heat_map_sum - tmp_sum_mid\n",
    "    \n",
    "    np.bitwise_and(pizza_heatmap,mask_dict[high],tmp_sum_heatmap)\n",
    "    uint_tmp_sum_heatmap_high = tmp_sum_heatmap.astype('uint8')\n",
    "    tmp_sum_high = np.sum(uint_tmp_sum_heatmap_high)\n",
    "    diff_high = half_heat_map_sum - tmp_sum_high\n",
    "    \n",
    "    print(\"Step {},difference mid : {} Angels: low: {}, mid: {} high: {}\".format(step,diff_mid,low,mid,high))\n",
    "    \n",
    "    if (diff_low == half_heat_map_sum):\n",
    "        mid = low\n",
    "        break\n",
    "    if (diff_mid == half_heat_map_sum):\n",
    "        break\n",
    "    \n",
    "    if (diff_high == half_heat_map_sum):\n",
    "        mid = high\n",
    "        break   \n",
    "\n",
    "    #check for \n",
    "    if diff_low > 0 and diff_mid < 0:\n",
    "        #set new search between low and mid\n",
    "        low = low\n",
    "        mid = int(low + ((high - low)/2))\n",
    "        high = mid \n",
    "        \n",
    "    else:\n",
    "        #set new search between mid and high\n",
    "        low = mid\n",
    "        mid = int(low + ((high - low)/2))\n",
    "        high = high\n",
    "    \n",
    "    step = step +1 \n",
    "    #check for finish\n",
    "    if(low + 1 == mid):\n",
    "        break\n",
    "    \n",
    "print(\"Found best angel at: {} diff: {}\".format(mid, diff_mid))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D pippa slice heatmap of test image\n",
    "\n",
    "![Slicing Maks](img/TestimageSliceHeatmap.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw the slice in 3d \n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.arange(0,320,1)\n",
    "y = np.arange(0,320,1)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "X.shape\n",
    "Z = uint_tmp_sum_heatmap_mid\n",
    "\n",
    "mycmap = plt.get_cmap('YlOrRd')\n",
    "ax1.set_title('Pepperoni slice heatmap')\n",
    "surf1 = ax1.plot_surface(X, Y, Z, cmap=mycmap)\n",
    "fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=5)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the slice angle\n",
    "mid = 45\n",
    "\n",
    "if mid <= 90:\n",
    "    # counter clockwise\n",
    "    out_cut_image = out_img_arrow.rotate(90 - mid)\n",
    "else:\n",
    "    out_cut_image = out_img_arrow.rotate(360 - (mid -90))\n",
    "\n",
    "outframe = hdmi_out.newframe()\n",
    "#Blend the two images\n",
    "slice_output = PIL.Image.blend(out_img_cut_here,out_cut_image,0.5)\n",
    "\n",
    "#Write out frame\n",
    "outframe = hdmi_out.newframe()\n",
    "cv2.cvtColor(np.array(slice_output), cv2.COLOR_RGB2BGR,dst=outframe)\n",
    "hdmi_out.writeframe(outframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
